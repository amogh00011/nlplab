{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc947ae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding:\n",
      "[[0 0 1 1 1 1 0 0 1 0 0 1 0]\n",
      " [0 0 1 1 0 1 0 0 1 1 0 1 0]\n",
      " [1 0 1 0 0 0 0 1 1 0 1 0 1]\n",
      " [1 1 1 0 1 1 1 0 1 0 0 0 0]]\n",
      "\n",
      "Bag of Words (BOW):\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "\n",
      "n-grams:\n",
      "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
      " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n",
      "\n",
      "Tf-Idf:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "\n",
      "Custom Features:\n",
      "[[27]\n",
      " [37]\n",
      " [26]\n",
      " [27]]\n",
      "\n",
      "Word2Vec (Word Embedding) Features:\n",
      "[[-0.01430683  0.00804962  0.01949739  0.00651358 -0.01567571 -0.00421082\n",
      "   0.00738333  0.00142617  0.00949908 -0.00872436  0.01341875  0.02308466\n",
      "  -0.01206369  0.00078877 -0.00692497  0.01177793 -0.02375894  0.01165193\n",
      "  -0.01417152 -0.0010905   0.02833861]\n",
      " [-0.01890265 -0.00683823  0.00221132 -0.00232065 -0.01331951 -0.01265449\n",
      "   0.00234983 -0.00076955  0.0078635  -0.01484946  0.01157213  0.01572549\n",
      "  -0.0033331   0.01688552  0.00316584  0.01023834 -0.02053705  0.00795649\n",
      "  -0.01356513 -0.00021084  0.01896621]\n",
      " [-0.00490512  0.00567745  0.00782229  0.00745345 -0.01216369 -0.00058014\n",
      "   0.01924429  0.01527272 -0.02781302 -0.01898776  0.0038893   0.00031558\n",
      "  -0.01065661 -0.00933871  0.00728949 -0.00327569  0.00823459  0.00789626\n",
      "  -0.01468172 -0.02617179  0.01789556]\n",
      " [-0.0204542   0.00344436  0.01067198  0.01756525 -0.01868036  0.00450339\n",
      "   0.01518335  0.01119797  0.00241141  0.0121401   0.02504327  0.00927749\n",
      "   0.01170818 -0.00400818  0.01111272 -0.01318253 -0.02241736  0.01646586\n",
      "  -0.00228773  0.01169194  0.01029818]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "# Sample text data\n",
    "text_data = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "    ]\n",
    "# i) One Hot Encoding\n",
    "def one_hot_encoding(text_data):\n",
    "    unique_words = set(\" \".join(text_data).split())\n",
    "    encoded_data = []\n",
    "    for text in text_data:\n",
    "        encoded_text = [1 if word in text else 0 for word in unique_words]\n",
    "        encoded_data.append(encoded_text)\n",
    "    return np.array(encoded_data)\n",
    "one_hot_encoded = one_hot_encoding(text_data)\n",
    "print(\"One Hot Encoding:\")\n",
    "print(one_hot_encoded)\n",
    "# ii) Bag of Words (BOW)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(text_data)\n",
    "print(\"\\nBag of Words (BOW):\")\n",
    "print(bow_features.toarray())\n",
    "# iii) n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_features = ngram_vectorizer.fit_transform(text_data)\n",
    "print(\"\\nn-grams:\")\n",
    "print(ngram_features.toarray())\n",
    "# iv) Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
    "print(\"\\nTf-Idf:\")\n",
    "print(tfidf_features.toarray())\n",
    "# v) Custom features (e.g., length of documents)\n",
    "custom_features = np.array([[len(doc)] for doc in text_data])\n",
    "print(\"\\nCustom Features:\")\n",
    "print(custom_features)\n",
    "# vi) Word2Vec (Word Embedding)\n",
    "word2vec_model = Word2Vec([doc.split() for doc in text_data], min_count=1)\n",
    "\n",
    "# Generate Word2Vec features\n",
    "word2vec_model = Word2Vec([doc.split() for doc in text_data], min_count=1, vector_size=21)\n",
    "\n",
    "# Generate Word2Vec features by averaging word vectors in each document\n",
    "word2vec_features = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in doc.split() if word in word2vec_model.wv], axis=0)\n",
    "    for doc in text_data\n",
    "])\n",
    "print(\"\\nWord2Vec (Word Embedding) Features:\")\n",
    "print(word2vec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795f746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
